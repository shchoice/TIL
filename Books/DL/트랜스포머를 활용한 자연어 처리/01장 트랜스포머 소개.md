# 1장. 트랜스포머 소개

## 개요

이 장에서는 다음 5가지에 대해 다루고 있음

1. 트랜스포머란 무엇인지
2. 트랜스포머의 중요한 3가지 기술 
3. 트랜스포머의 기초가 되는 핵심 개념 
4. 어떤 종류의 작업을 잘 처리하는지 
5. 허깅페이스 생태계의 도구와 라이브러리

## 트랜스포머란
- 2017년에 구글이 소개한 논문 "Attention is All You Need"에서 처음 제안된 신경망 아키텍처
- 시퀀스 모델링(sequence modeling)을 위한 새로운 신경망 아키텍처로 NLP 분야에서 매우 효과적
- 기존의 RNN, LSTM은 sequence data를 처리할 때 순차적으로 데이터를 처리하는 방식을 사용했는데 학습 시간이 오래 걸리고 장기적인 의존성을 학습하는 데 어려움이 있었으나
- 트랜스포머는 어텐션 메커니즘을 도입하여 모델이 입력 시퀀스의 모든 단어에 대해 병렬적으로 주의를 기울여, 모델은 각 단어가 다른 단어와 어떻게 연관되어 있는지를 배울 수 있고 이를 통해 문맥을 이해하고 문장의 의미를 파악하는 데 효과적
- 트랜스포머는 이러한 어텐션 메커니즘을 `셀프 어텐션` 또는 `트랜스포머 어텐션` 이라고 부르는 고유한 방식을 사용
- 셀프 어텐션은 입력 시퀀스의 모든 위치에서 모든 위치로 정보를 이동시킬 수 있도록 해서, 장기적인 의존성을 효과적으로 학습하는 데 도움이 됨
- 오늘날 가장 유명한 두 트랜스포머
  - GPT(Generative Pretrained Transformers)
  - BERT(Bidirectional Encoder Representations from Transformers)

<img alt="transformer-timeline" caption="The transformers timeline" src="https://user-images.githubusercontent.com/100276387/240818612-54b9b038-629a-4df2-9722-d9bc9948672e.png" id="transformer-timeline"/>



## 트랜스포머의 중요한 3가지 기술 

### 인코더-디코더 프레임워크

* Seq2Seq 모델에서 주로 사용되는 구조로  인코더와 디코더로 구성이 됨
* 인코더는 입력데이터를 고차원의 표현 형태로 인코딩
  * 기계 번역을 예로 하면 입력 문장("What is your name")을 인코더의 input으로 사용하면 이 문장은 내부적인 표현 즉 일종의 벡터로 변환이 되며, 문장의 모든 의미와 구문 정보를 인코딩하게 됨
* 디코더는 이 표현을 사용하여 원하는 출력을 생성
  * 디코더는 인코더의 내부 표현(벡터)를 받아서 이를 대상 언어("이름이 뭐에요?")로 변환
  * 이 과정은 일반적으로 순차적으로 이루엉짐
  * 각 시점에서 디코더는 지금까지 생성된 출력을 고려하여 다음 단어를 생성
* 트랜스포머에서는 각 인코더와 디코더가 여러 층으로 쌓여 있으며, 각 층은 셀프 어텐션 및 피드 포워드 신경망으로 구성



### 어텐션 메커니즘

* 모델의 입력의 각 부분에 대해 얼마나 주의를 기울여야 하는지를 결정하는 방법

* 트랜스 포머에서는 셀프 어텐션(Self-Attention 또는 Scaled dot-product attention)이라는 특별한 형태의 어텐션 메커니즘을 사용

  * 셀프 어텐션은 모든 위치에서 모든 위치로 정보를 전달할 수 있게 해서 입력 시퀀스의 각 부분이 다른 부분과 어떻게 관련되어 있는지를 모델이 이해하도록 도움
  * 즉, 시퀀스 내의 각 원소(ex. 문장 내의 각 단어)가 다른 원소와 어떻게 관련되어 있는지를 파악하는 방식
  * 셀프 어텐션을 통해 모델은 문장 내의 각 단어가 다른 단어들과 어떻게 상호 작용하는지 이해할 수 있게 함

* 트랜스포머의 어텐션 메커니즘을 통해 생성된 벡터는 입력 문장의 모든 단어와의 관계를 고려하여 단어의 '동적' 의미를 반영. 즉, 같은 단어라도 그 주변 단어(즉, 문맥)에 따라 다른 벡터를 가질 수 있음

  * 원본 단어 벡터(워드 임베딩)는 보통 단어의 '정적' 의미를 반영 -> 사과의 경우 먹는 사과만 이해하고, 화해의 사과는 이해를 못함.

* 쿼리(Query), 키(Key), 값(Value)

  * Query

    * 주어진 문제나 질문을 표현하는 벡터로, 현재 처리하고 있는 단어나 문장의 정보를 포함하여 어떤 정보를 찾아야 하는지를 나타냄
    * 원문의 단어 벡터에 가중치 행렬을 곱해 Query 벡터를 생성(벡터와 행렬의 곱셈 연산)
      * 예) 단어 벡터가 [1,2,3] 이라는 3차원 벡터, 가중치 행렬  [[1, 2, 3], [4, 5, 6], [7, 8, 9]] 이라면
        [1 * 1 + 2 * 4 + 3 * 7, 1 * 2 + 2 * 5 + 3 * 8, 1 * 3 + 2 * 6 + 3 * 9] = [30, 36, 42]

  * Key

    * 원문의 각 단어를 표현하는 벡터로 특정 단어의 문맥적 정보를 담고 있음

    * Query와 각 Key 사이의 유사도를 계산하여 어떤 단어들에 집중해야 하는지를 결정(어떤 단어들이 Query와 가장 관련이 있는지를 결정)

      > Ex. "펭귄은 날지 못하지만 빠르게 수영할 수 있다"라는 문장
      >
      > * 여기서 "수영할"이라는 Query가 주어졌을 때, "펭귄은", "날지", "못하지만", "빠르게", "수영할", "수", "있다" 각각에 대한 Key가 있을 것
      > * 이 Key들은 각 단어의 문맥적 정보를 포함하며, Query인 "수영할"과 얼마나 관련이 있는지를 판단하는 역할
      > * 어텐션 메커니즘은 Query인 "수영할"이 각 단어와 얼마나 관련이 있는지를 판단하기 위해 Query와 각 Key 벡터의 유사도를 계산
      > * 이 계산 결과는 각 단어에 대한 가중치로 사용되며, 이 가중치는 해당 단어가 Query인 "수영할"과 얼마나 관련이 있는지를 나타냄
      > * "펭귄은"이나 "빠르게"는 "수영할"과 직접적인 관련이 있기 때문에 높은 가중치를 가질 것이고, "날지"나 "못하지만"은 "수영할"과 관련이 덜하기 때문에 낮은 가중치를 가질 것
      > * 그 후 이 가중치들은 각 단어의 Value 벡터와 결합하여 최종 출력을 생성합니다. 이렇게 생성된 출력 벡터는 Query인 "수영할"에 대한 문맥적 표현을 제공
      > * 어텐션 메커니즘은 문장의 각 단어에 대해 어떤 단어에 주목해야 하는지를 결정하고, 그 단어의 문맥적 정보를 포함하는 출력 벡터를 생성하는 역할

    * Key에 대한 가중치 행렬이 [[1, 0, 0], [0, 1, 0], [0, 0, 1]]이라면, 동일한 단어 벡터 [1, 2, 3]은 그대로 [1, 2, 3]을 Key 벡터로 얻게 됨

  * Value

    * 원문의 각 단어를 표현하는 벡터로 그 단어가 문장 속에서 어떤 역할과 의미를 가지는지를 나타냄

    * Key와 관련성이 높은 단어의 Value는 최종 출력에 더 큰 기여

      > Ex. 나는 강아지를 좋아한다"라는 문장
      >
      > * "강아지를"이라는 단어의 Value 벡터는 그 단어가 전체 문장에서 어떤 역할을 하는지를 나타냄
      > * 즉 주어인 "나는"이 "좋아한다"하는 대상이 "강아지"라는 정보를 담음

      > Ex. "펭귄은 날지 못하지만 빠르게 수영할 수 있다"라는 문장
      >
      > * 각 단어 "펭귄은", "날지", "못하지만", "빠르게", "수영할", "수", "있다"에 대한 Value 벡터는 각 단어가 문장에서 가지는 의미와 문맥을 반영
      > * "펭귄은"이라는 단어의 Value 벡터는 주어로서의 역할과 '펭귄'이라는 특정 생물을 가리키는 의미를 담음
      > * "수영할"이라는 단어의 Value 벡터는 동사로서의 역할과 '수영'이라는 특정 행동을 가리키는 의미를 담음
      > * 어텐션 메커니즘은 Query와 Key 벡터 간의 유사도를 기반으로 한 가중치를 계산하고 이 가중치를 각 Value 벡터에 적용하여 가중합을 구함
      > * 이렇게 계산된 가중합은 Query 단어에 대한 문맥적 정보를 반영한 새로운 벡터, 즉 새로운 단어 표현을 생성하는데 사용
      > * 즉, "수영할"이라는 Query에 대해, 각 단어의 Key 벡터와의 유사도에 따라 "펭귄은", "날지", "못하지만", "빠르게", "수영할", "수", "있다" 각 단어의 Value 벡터에 가중치를 부여하고 이들을 합산해 최종적으로 "수영할"에 대한 문맥을 반영한 새로운 표현을 생성

    * Value에 대한 가중치 행렬이 [[1, 1, 1], [2, 2, 2], [3, 3, 3]]이라면, 동일한 단어 벡터 [1, 2, 3]은 [6, 12, 18]을 Value 벡터로 얻게 됨

* 작동 원리

  * 쿼리(Query), 키(Key), 값(Value) 생성
    * 입력 시퀀스(ex. 문장, 원문의 단어 벡터(워드 임베딩))의 각 단어에 대해 Query, Key, Value라는 세 가지 벡터가 생성됨
    * 이 벡터들은 원본 단어 벡터에 가중치 행렬을 곱하여 계산됨
  * Attention score 계산
    * 각 단어에 대해 Query 벡터와 모든 Key 벡터 사이의 dot product(점곱, 내적)을 계산
    * dot product는 각 단어가 다른 단어와 얼마나 관련이 있는지를 나타내는 attention score를 생성
    * 즉, attention score는 특정 단어(쿼리)가 문맥 내의 다른 단어들(keys)에 얼마나 `집중`해야하는지를 결정
  * 확률 분포 생성
    * attention score들을 softmax 함수를 통해 확률 분포로 변환
    * 이 확률 분포는  어떤 단어들에 더 많은 주의를 기울여야 하는지를 나타냄
  * 가중된 합 계산
    * Value 벡터들을 앞서 계산한 확률 분포를 이용해 가중치를 부여하고 합산(각 Value 벡터에 대해 앞서 계산한 확률 값(가중치)를 곱한 후, 이들을 모두 합산하는 것을 의미)
      * 좀 더 상세히 말하자면..
        * Query 벡터와 각 Key 벡터의 유사도를 계산하고, 이 유사도를 확률 값으로 변환
          * 이 확률 값은 각 Value 벡터가 최종 출력에 얼마나 많은 기여를 할지를 결정하는 가중치 역할
        * 특정 key 벡터와 Query 벡터가 매우 유사하다면, 이 Key 벡터에 대응하는 Value 벡터는 높은 가중치를 받게 됨, 반면 많이 다르다면, 해당 key에 대응하는 value 벡터는 낮은 가중치를 받게 될 것
        * 이렇게 각 Valuye 벡터에 가중치를 곱한 후,  이들을 모두 합산하여 최종 출력 벡터를 만듬
        * 이 출력 벡터는 각 Value 벡터의 가중 평균을 나타내며, 이는 주어진 Query에 대해 가장 관련한 정보를 제공
      * 이러한 가중된 합산 과정은 입력 문장의 각 단어가 다른 단어와 어떻게 연관되어 있는지를 반영한, 각 단어의 새로운 표현을 만드는 데 중요, 이 표현은 원본 단어 벡터보다 문맥적 정보를 더 잘 반영할 수 있음.
    * 이렇게 생성된 출력 벡터는 원래의 단어에 대한 새로운 표현으로, 문장의 모든 단어와의 관계를 반영
