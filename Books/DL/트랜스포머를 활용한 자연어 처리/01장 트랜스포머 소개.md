# 1장. 트랜스포머 소개

## 개요

이 장에서는 다음 5가지에 대해 다루고 있음

1. 트랜스포머란 무엇인지
2. 트랜스포머의 중요한 3가지 기술 
3. 트랜스포머의 기초가 되는 핵심 개념 
4. 어떤 종류의 작업을 잘 처리하는지 
5. 허깅페이스 생태계의 도구와 라이브러리

## 트랜스포머란
- 2017년에 구글이 소개한 논문 "Attention is All You Need"에서 처음 제안된 신경망 아키텍처
- 시퀀스 모델링(sequence modeling)을 위한 새로운 신경망 아키텍처로 NLP 분야에서 매우 효과적
- 기존의 RNN, LSTM은 sequence data를 처리할 때 순차적으로 데이터를 처리하는 방식을 사용했는데 학습 시간이 오래 걸리고 장기적인 의존성을 학습하는 데 어려움이 있었으나
- 트랜스포머는 어텐션 메커니즘을 도입하여 모델이 입력 시퀀스의 모든 단어에 대해 병렬적으로 주의를 기울여, 모델은 각 단어가 다른 단어와 어떻게 연관되어 있는지를 배울 수 있고 이를 통해 문맥을 이해하고 문장의 의미를 파악하는 데 효과적
- 트랜스포머는 이러한 어텐션 메커니즘을 `셀프 어텐션` 또는 `트랜스포머 어텐션` 이라고 부르는 고유한 방식을 사용
- 셀프 어텐션은 입력 시퀀스의 모든 위치에서 모든 위치로 정보를 이동시킬 수 있도록 해서, 장기적인 의존성을 효과적으로 학습하는 데 도움이 됨
- 오늘날 가장 유명한 두 트랜스포머
  - GPT(Generative Pretrained Transformers)
  - BERT(Bidirectional Encoder Representations from Transformers)

<img alt="transformer-timeline" caption="The transformers timeline" src="https://user-images.githubusercontent.com/100276387/240818612-54b9b038-629a-4df2-9722-d9bc9948672e.png" id="transformer-timeline"/>



## 트랜스포머의 중요한 3가지 기술 

### 인코더-디코더 프레임워크

* Seq2Seq 모델에서 주로 사용되는 구조로  인코더와 디코더로 구성이 됨
* 인코더는 입력데이터를 고차원의 표현 형태로 인코딩
  * 기계 번역을 예로 하면 입력 문장("What is your name")을 인코더의 input으로 사용하면 이 문장은 내부적인 표현 즉 일종의 벡터로 변환이 되며, 문장의 모든 의미와 구문 정보를 인코딩하게 됨
* 디코더는 이 표현을 사용하여 원하는 출력을 생성
  * 디코더는 인코더의 내부 표현(벡터)를 받아서 이를 대상 언어("이름이 뭐에요?")로 변환
  * 이 과정은 일반적으로 순차적으로 이루엉짐
  * 각 시점에서 디코더는 지금까지 생성된 출력을 고려하여 다음 단어를 생성
* 트랜스포머에서는 각 인코더와 디코더가 여러 층으로 쌓여 있으며, 각 층은 셀프 어텐션 및 피드 포워드 신경망으로 구성

