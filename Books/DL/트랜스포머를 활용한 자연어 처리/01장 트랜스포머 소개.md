* # 1장. 트랜스포머 소개

  ## 개요
  
  이 장에서는 다음 5가지에 대해 다루고 있음
  
  1. 트랜스포머란 무엇인지
  2. 트랜스포머의 중요한 3가지 기술 
  3. 트랜스포머의 기초가 되는 핵심 개념 
  4. 어떤 종류의 작업을 잘 처리하는지 
  5. 허깅페이스 생태계의 도구와 라이브러리
  
  ## 트랜스포머란
  
  - 2017년에 구글이 소개한 논문 "Attention is All You Need"에서 처음 제안된 신경망 아키텍처
  - 시퀀스 모델링(sequence modeling)을 위한 새로운 신경망 아키텍처로 NLP 분야에서 매우 효과적
  - 기존의 RNN, LSTM은 sequence data를 처리할 때 순차적으로 데이터를 처리하는 방식을 사용했는데 학습 시간이 오래 걸리고 장기적인 의존성을 학습하는 데 어려움이 있었으나
  - 트랜스포머는 어텐션 메커니즘을 도입하여 모델이 입력 시퀀스의 모든 단어에 대해 병렬적으로 주의를 기울여, 모델은 각 단어가 다른 단어와 어떻게 연관되어 있는지를 배울 수 있고 이를 통해 문맥을 이해하고 문장의 의미를 파악하는 데 효과적
  - 트랜스포머는 이러한 어텐션 메커니즘을 `셀프 어텐션` 또는 `트랜스포머 어텐션` 이라고 부르는 고유한 방식을 사용
  - 셀프 어텐션은 입력 시퀀스의 모든 위치에서 모든 위치로 정보를 이동시킬 수 있도록 해서, 장기적인 의존성을 효과적으로 학습하는 데 도움이 됨
  - 오늘날 가장 유명한 두 트랜스포머
    - GPT(Generative Pretrained Transformers)
    - BERT(Bidirectional Encoder Representations from Transformers)
  
  <img alt="transformer-timeline" caption="The transformers timeline" src="https://user-images.githubusercontent.com/100276387/240818612-54b9b038-629a-4df2-9722-d9bc9948672e.png" id="transformer-timeline"/>
  
  
  
  ## 트랜스포머의 중요한 3가지 기술 
  
  ### 인코더-디코더 프레임워크
  
  1. #### 인코더-디코더 구조와 RNN
  
  * Seq2Seq 모델에서 인코더-디코더 구조가 자주 사용
  
    * RNN(Recurrent Neural Network)은 이 구조의 주요 구성 요소로, 단어나 문자와 같은 입력을 받아 `은닉 상태(hidden state)`라는 벡터를 출력

    * 이 출력된 정보는 바로 다음 입력으로 넘겨져서 네트워크가 자기 자신에 대한 정보를 활용하게 됨
      이런 방식으로 RNN은 이전 단계의 정보를 추적하며 예측을 생성
  
    * RNN은 단어 시퀀스를 한 언어에서 다른 언어로 매핑하는 기계 번역 시스템에서 중요한 역할
      이런 작업은 대체로 인코더-디코더(Seq2Seq) 구조로 처리
      인코더는 입력 시퀀스 정보를 `마지막 은닉 상태`라는 수치로 인코딩하고, 이 상태는 디코더로 전달되어 출력 시퀀스를 생성
  
      
  
      ![rnn](https://user-images.githubusercontent.com/100276387/240818662-f72b2b05-4ca4-4ced-b552-08ebdebe6e47.png)
  
  
  2. #### 인코더와 디코더의 역할
  
  * 인코더는 입력 데이터를 고차원의 표현 형태로 변환(인코딩)하는 역할을 함
    * 기계 번역을 예로 하면 입력 문장("What is your name")을 인코더의 input으로 사용하면 이 문장은 내부적인 표현 즉 일종의 벡터로 변환이 되며, 문장의 모든 의미와 구문 정보를 인코딩하게 됨
  * 디코더는 인코더의 내부 표현(벡터)을 사용하여 원하는 출력을 생성
    * 디코더는 인코더의 내부 표현(벡터)를 받아서 이를 대상 언어("이름이 뭐에요?")로 변환
    * 이 과정은 일반적으로 순차적으로 이루어짐
    * 각 시점에서 디코더는 지금까지 생성된 출력을 고려하여 다음 단어를 생성
  * 일반적으로 인코더와 디코더는 시퀀스를 모델링할 수 있는 어떤 종류의 신경망도 가능
  
    * 영어 문장 Transformer are great! 을 은닉 상태 벡터로 인코딩한 다음, 이를 디코딩해 독일어 문장Transformer sind grossartig!으로 만든 한쌍의 RNN 예시
      ![enc-dec](https://user-images.githubusercontent.com/100276387/240818845-616671d5-f1e1-4ca3-a5da-a1a26b67c2f5.png)
      * 이 구조는 인코더의 마지막 은닉 상태가 `정보 병목(Information bottleneck)`이 된다는 약점을 가지고 있음
        * 입력 시퀀스의 길이에 관계 없이 모든 정보를 단일 은닉 상태에 저장해야 하기 때문
        * 시퀀스가 긴 경우, 모든 것을 고정된 하나의 표현으로 압축하는 과정에서 시작 부분의 정보가 손실될 가능성이 있어 더욱 취약
      * 따라서 어텐션(Attention) 이 필요!!
        * 디코더는 인코더의 마지막 은닉 상태만을 참조해 출력을 만드므로 여기에 전체 입력 시퀀스의 의미가 담겨야 함
        * 다행히 디코더가 인코더의 모든 은닉 상태에 접근해 이 병목을 제거하는데 이런 일반적인 메커니즘을 `어텐션(attention)` 이라 하는데 이는 최신 신경망 구조의 핵심 구성 요소
      * RNN을 위한 어텐션을 만드는 방법을 이해하면 트랜스포머 아키텍처의 주요 구성 요소도 잘 이해할 수 있게됨
  
  #### 3. 트랜스포머의 인코더-디코더 구조
  
  * 트랜스포머에서는 각 인코더와 디코더가 여러 층으로 쌓여 있으며, 각 층은 셀프 어텐션 및 피드 포워드 신경망으로 구성
