* * * # 1장. 트랜스포머 소개

      ## 개요
  
      이 장에서는 다음 5가지에 대해 다루고 있음
  
      1. 트랜스포머란 무엇인지
      2. 트랜스포머의 중요한 3가지 기술 
      3. 트랜스포머의 기초가 되는 핵심 개념 
      4. 어떤 종류의 작업을 잘 처리하는지 
      5. 허깅페이스 생태계의 도구와 라이브러리
  
      ## 트랜스포머란
  
      - 2017년에 구글이 소개한 논문 "Attention is All You Need"에서 처음 제안된 신경망 아키텍처
      - 시퀀스 모델링(sequence modeling)을 위한 새로운 신경망 아키텍처로 NLP 분야에서 매우 효과적
      - 기존의 RNN, LSTM은 sequence data를 처리할 때 순차적으로 데이터를 처리하는 방식을 사용했는데 학습 시간이 오래 걸리고 장기적인 의존성을 학습하는 데 어려움이 있었으나
      - 트랜스포머는 어텐션 메커니즘을 도입하여 모델이 입력 시퀀스의 모든 단어에 대해 병렬적으로 주의를 기울여, 모델은 각 단어가 다른 단어와 어떻게 연관되어 있는지를 배울 수 있고 이를 통해 문맥을 이해하고 문장의 의미를 파악하는 데 효과적
      - 트랜스포머는 이러한 어텐션 메커니즘을 `셀프 어텐션` 또는 `트랜스포머 어텐션` 이라고 부르는 고유한 방식을 사용
      - 셀프 어텐션은 입력 시퀀스의 모든 위치에서 모든 위치로 정보를 이동시킬 수 있도록 해서, 장기적인 의존성을 효과적으로 학습하는 데 도움이 됨
      - 오늘날 가장 유명한 두 트랜스포머
        - GPT(Generative Pretrained Transformers)
        - BERT(Bidirectional Encoder Representations from Transformers)
  
      <img alt="transformer-timeline" caption="The transformers timeline" src="https://user-images.githubusercontent.com/100276387/240818612-54b9b038-629a-4df2-9722-d9bc9948672e.png" id="transformer-timeline"/>
  
      
  
      ## 트랜스포머의 중요한 3가지 기술 
  
      ### 인코더-디코더 프레임워크
  
      1. #### 인코더-디코더 구조와 RNN
  
      * Seq2Seq 모델에서 인코더-디코더 구조가 자주 사용
  
        * RNN(Recurrent Neural Network)은 이 구조의 주요 구성 요소로, 단어나 문자와 같은 입력을 받아 `은닉 상태(hidden state)`라는 벡터를 출력

        * 이 출력된 정보는 바로 다음 입력으로 넘겨져서 네트워크가 자기 자신에 대한 정보를 활용하게 됨
          이런 방식으로 RNN은 이전 단계의 정보를 추적하며 예측을 생성
  
        * RNN은 단어 시퀀스를 한 언어에서 다른 언어로 매핑하는 기계 번역 시스템에서 중요한 역할
          이런 작업은 대체로 인코더-디코더(Seq2Seq) 구조로 처리
          인코더는 입력 시퀀스 정보를 `마지막 은닉 상태`라는 수치로 인코딩하고, 이 상태는 디코더로 전달되어 출력 시퀀스를 생성
  
          
  
          ![rnn](https://user-images.githubusercontent.com/100276387/240818662-f72b2b05-4ca4-4ced-b552-08ebdebe6e47.png)
  
  
      2. #### 인코더와 디코더의 역할
  
      * 인코더는 입력 데이터를 고차원의 표현 형태로 변환(인코딩)하는 역할을 함
        * 기계 번역을 예로 하면 입력 문장("What is your name")을 인코더의 input으로 사용하면 이 문장은 내부적인 표현 즉 일종의 벡터로 변환이 되며, 문장의 모든 의미와 구문 정보를 인코딩하게 됨
      * 디코더는 인코더의 내부 표현(벡터)을 사용하여 원하는 출력을 생성
        * 디코더는 인코더의 내부 표현(벡터)를 받아서 이를 대상 언어("이름이 뭐에요?")로 변환
        * 이 과정은 일반적으로 순차적으로 이루어짐
        * 각 시점에서 디코더는 지금까지 생성된 출력을 고려하여 다음 단어를 생성
      * 일반적으로 인코더와 디코더는 시퀀스를 모델링할 수 있는 어떤 종류의 신경망도 가능
  
        * 영어 문장 Transformer are great! 을 은닉 상태 벡터로 인코딩한 다음, 이를 디코딩해 독일어 문장Transformer sind grossartig!으로 만든 한쌍의 RNN 예시
          ![enc-dec](https://user-images.githubusercontent.com/100276387/240818845-616671d5-f1e1-4ca3-a5da-a1a26b67c2f5.png)
          * 이 구조는 인코더의 마지막 은닉 상태가 `정보 병목(Information bottleneck)`이 된다는 약점을 가지고 있음
            * 입력 시퀀스의 길이에 관계 없이 모든 정보를 단일 은닉 상태에 저장해야 하기 때문
            * 시퀀스가 긴 경우, 모든 것을 고정된 하나의 표현으로 압축하는 과정에서 시작 부분의 정보가 손실될 가능성이 있어 더욱 취약
          * 따라서 어텐션(Attention) 이 필요!!
            * 디코더는 인코더의 마지막 은닉 상태만을 참조해 출력을 만드므로 여기에 전체 입력 시퀀스의 의미가 담겨야 함
            * 다행히 디코더가 인코더의 모든 은닉 상태에 접근해 이 병목을 제거하는데 이런 일반적인 메커니즘을 `어텐션(attention)` 이라 하는데 이는 최신 신경망 구조의 핵심 구성 요소
          * RNN을 위한 어텐션을 만드는 방법을 이해하면 트랜스포머 아키텍처의 주요 구성 요소도 잘 이해할 수 있게됨
  
      #### 3. 트랜스포머의 인코더-디코더 구조
  
      * 트랜스포머에서는 각 인코더와 디코더가 여러 층으로 쌓여 있으며, 각 층은 셀프 어텐션 및 피드 포워드 신경망으로 구성
  
      ### 어텐션 메커니즘
  
      * 어텐션
  
        * 어텐션은 입력 시퀀스에서 은닉 상태를 만들지 않고 스텝마다 인코더에서 디코더가 참고할 은닉 상태를  출력한다는 주요 개념에 기초
  
        * 하지만 모든 상태를 동시에 사용하려면 디코더에 많은 입력이 발생하므로 어떤 상태를  먼저 사용할지 우선순위를 정하는 메커니즘이 필요. -> 어텐션!
  
        * 디코더가 모든 디코딩 timestamp마다 인코더의 각 상태에 따른 가중치 또는 `어텐션`을 할당
  
          ![enc-dec-attn](https://user-images.githubusercontent.com/100276387/240818989-12693df2-246e-402e-81df-46a2b9f98974.png)
  
        * 어텐션 기반 모델은 timestamp마다 가장 많이 관련된 입력 토큰에 초점을 맞추므로 번역 문장에 있는 단어와 원 문장에 있는 단어의 복잡한 정렬 문제를 학습
  
          * 예를 들어 아래의 그림은 영어-프랑스어 번역 모델의 어텐션 가중치를 보여줌
  
            * 프랑스어 zone과 동일한 다섯번째 토큰은 European이지만 일곱 번째 토큰인 Area와의 가중치가 높은 값(밝은 색)을 갖음
  
            ![attention-alignment](https://user-images.githubusercontent.com/100276387/241863919-542be7c7-4a64-419d-aa98-9cfd738fb652.png)
  
        * 어텐션으로 번역이 한결 좋아졌지만, 인코더와 디코더에 사용하는 순환 모델의 단점은 여전히 존재
          즉, 태생적으로 계산이 순차적으로 수행되며 입력 시퀀스 전체에 걸쳐 병렬화할 수 없음
  
      * 셀프 어텐션(Self-attention)
  
        * 트랜스포머는 모델링 패러다임을 순환을 모두 없앤 `셀프 어텐션(Self-attention)`으로 바꿈
  
        * 신경망의 같은 층에 있는 모든 상태에 대해서 어텐션을 작동시키는 방식
  
        * 인코더와 디코더는 각각 셀프 어텐션을 갖고 있으며 어텐션의 출력은 feed-forward neural network(FF NN)에 주입됨
  
        * 셀프 어텐션 구조는 순환 모델보다 훨씬 더 빠르게 훈련하며 최근 NLP 분야에서 대단한 혁신을 일으킴
  
          ![transformer-self-attn](https://user-images.githubusercontent.com/100276387/241861898-a19b31dd-27f2-4ca5-ac84-825c869b60e3.png)
  
      * 트랜스포머 논문에서는 처음부터 다양한 언어의 문장 쌍으로 구성된 대규모 말뭉치에서 번역 모델을 훈련했으나  모델 훈련에 사용할 레이블링된 대규모 텍스트 데이터를 구하기 어렵기 때무문에 트랜스포머 혁명을 시작하는 데 필요했던 마지막 퍼즐 조각은 전이 학습 이었음
  
      ## NLP의 전이학습
  
      * 컴퓨터 비전에서는 전이 학습이 표준이 됐지만 NLP에서는 2017-18년에 전이 학습을 수행하는 방식을 제안
  
        * ULMFiT 프레임워크의 등장
  
      * ULFMIT
  
        * 세 개의 주요 단계로 구성
  
          * 사전 훈련(Pretraining)
            * `언어 모델링(Language Modeling)` 작업을 수행
              * 이전 단어를 바탕으로 다음 단어를 예측하도록 목표함
              * 레이블링된 데이터가 필요하지 않으며 위키피디아 같은 소스에 있는 풍부한 텍스트를 활용
          * 도메인 적응
            * 언어 모델을 댁모 말뭉치에서 사전 훈련한 후, 다음 단계로 도메인 내 말뭉치에 적응시킴
              즉, 위키피디아에서 훈련한 모델을 IMDb 영화 리뷰 말뭉치에 적응을 시키는 것을 예로 할 수 있음
            * 여전히 언어 모델링을 사용하지만, 이제 모델은 Target 말뭉치(IMDb 영화 리뷰) 에 있는 다음 단어를 예측
          * 미세 튜닝(Finetuning) 
            * 언어 모델을 타깃 작업을 하기 위한 분류 층과 함께 미세튜닝
  
          ![ulmfit](https://user-images.githubusercontent.com/100276387/241861963-0d0d6f23-0328-4c95-913a-c11f22c3bcde.png)
  
      * 모델은 구조적으로 바디(Body)와 헤드(Head)로 나뉨
  
        * Body의 가중치는 훈련하는 동안 원래 Domain에서 다양한 특성을 학습하고, 이 가중치(학습 가능한 파라미터)를 사용해 새로운 작업을 위한 모델을 초기화
  
        * 적은 양의 레이블 데이터로 훨씬 효과적으로 훈련하는 높은 품질의 모델을 만듦
  
          ![transfer-learning](https://user-images.githubusercontent.com/100276387/241861954-aad0239e-2eb9-43a8-8647-75b1e536050b.png)
  
    
  
    ## 대표적 트랜스포머 모델
  
    2018년에 셀프 어텐션과 전이 학습을 결합한 두 개의 트랜스포머 모델(GPT, BERT)이 릴리즈 됨
  
    * GPT
      * 트랜스포머 아키텍처의 디코더 부분만 사용
      * ULMFiT 같은 언어 모델링 방법을 사용
    * BERT
      * 트랜스포머 아키턱처의 인코더 부분을 사용
      * MLM(Maksed language modeling)이라는 특별한 형태의 언어 모델링을 사용
        * 텍스트에서 랜덤하게 마스킹된 단어를 예측
          * I looked at my [MASK] and saw that [MASK] was late'라는 문장에서, 모델은 [MASK]로 마스킹된 단어에 대해 가장 가능성이 높은 후보를 예측
  
    GPT와 BERT는 트랜스포머 시대를 열었으나, 연구실마다 서로 호환되지 않는 프레임워크(PyTorch, Tensorflow)를 사용해 모델을 릴리즈 했고, 이런 모델을 NLP 기술자들이 자신의 애플리케이션으로 포팅하기 쉽지 않았음
  
    하지만 🤗Hugging Face Transformers 되면서 단일화된 API가 구축
